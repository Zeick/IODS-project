# Chapter 2: Week 2 assignment solutions

## EXERCISE 1
```{r eval=FALSE}
lrn14 = read.csv('learning2014.csv') # This is after the data wrangling part of the exercise is completed
dim(lrn14)
str(lrn14) # 7 variables and 166 persons who answered the questions
```
- The data records the age and gender of the people.
- Points: The exam point the people got (between 7 and 33)
- Deep/Stra/Surf: An average of 8-12 questions, each with 1-5 scale. They tell the preference of the student to deep, strategic or surface thinking.

Here's the summary:
```
X          gender       Age           attitude          deep      
 Min.   :  1.00   F:110   Min.   :17.00   Min.   :1.400   Min.   :1.583  
 1st Qu.: 42.25   M: 56   1st Qu.:21.00   1st Qu.:2.600   1st Qu.:3.333  
 Median : 83.50           Median :22.00   Median :3.200   Median :3.667  
 Mean   : 83.50           Mean   :25.51   Mean   :3.143   Mean   :3.680  
 3rd Qu.:124.75           3rd Qu.:27.00   3rd Qu.:3.700   3rd Qu.:4.083  
 Max.   :166.00           Max.   :55.00   Max.   :5.000   Max.   :4.917  
      stra            surf           Points     
 Min.   :1.250   Min.   :1.583   Min.   : 7.00  
 1st Qu.:2.625   1st Qu.:2.417   1st Qu.:19.00  
 Median :3.188   Median :2.833   Median :23.00  
 Mean   :3.121   Mean   :2.787   Mean   :22.72  
 3rd Qu.:3.625   3rd Qu.:3.167   3rd Qu.:27.75  
 Max.   :5.000   Max.   :4.333   Max.   :33.00  
 ```
## EXERCISE 2
```{r eval=FALSE}
library(GGally)
library(ggplot2)
summary(lrn14)
pairs(lrn14[-1],col=lrn14$gender) # It's hard to see anything from this
```
[The plot](Overview.png) is very hard to interpret. [Here's a more advanced plot matrix](Overview2.png) with ggpairs(), where you can actually see something.
```{r eval=FALSE}
p <- ggpairs(lrn14, mapping = aes(col=gender,alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

- Women have red color. Men have blue color. There are more women than men.
- Age of both men and women are approximately the same.
- The attitude appears to be slightly better with men.
- Deep learning is the same with men and women
- Strategic learning could be a little bit better with women, also surface lerning. The boxplots are at almost same position, so the difference appears to be negligible.
- Points are the same for both men and women.

## EXERCISE 3

Regression model for Points with explanatory variables deep, stra and surf
```{r eval=FALSE}
my_model2 <- lm(Points ~ deep + stra + surf, data = lrn14)
summary(my_model2)
```
Here's the output:
```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  26.9143     5.1169   5.260  4.5e-07 ***
deep         -0.7443     0.8662  -0.859   0.3915    
stra          0.9878     0.5962   1.657   0.0994 .  
surf         -1.6296     0.9153  -1.780   0.0769 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.827 on 162 degrees of freedom
Multiple R-squared:  0.04071,	Adjusted R-squared:  0.02295 
F-statistic: 2.292 on 3 and 162 DF,  p-value: 0.08016
```

According to summary of my model, the Student t-test suggests that the variables I chose (stra and surf) are significant, but not deep, since its t-value is by its magnitude almost the same as the standard error. I choose new explanatory variables: attitude, stra and surf
```{r eval=FALSE}
my_model2 <- lm(Points ~ attitude + stra + surf, data = lrn14)
summary(my_model2)
```
Here's the output:
```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  11.0171     3.6837   2.991  0.00322 ** 
attitude      3.3952     0.5741   5.913 1.93e-08 ***
stra          0.8531     0.5416   1.575  0.11716    
surf         -0.5861     0.8014  -0.731  0.46563    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

Stra and surf are now unsignificant! Okay, new plan. I try only attitude, with the only super-siginificant role.
```{r eval=FALSE}
my_model2 <- lm(Points ~ attitude, data = lrn14)
summary(my_model2)
```
Here's the output:
```
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  11.6372     1.8303   6.358 1.95e-09 ***
attitude      3.5255     0.5674   6.214 4.12e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.32 on 164 degrees of freedom
Multiple R-squared:  0.1906,	Adjusted R-squared:  0.1856 
F-statistic: 38.61 on 1 and 164 DF,  p-value: 4.119e-09
```
Looks much better, the Points' dependence of attitude is clear.

## EXERCISE 4
I interpret the points are highly dependent on attitude, and only weakly dependent on the learning techniques.

The linear model is like this:
```
Points = 3.53*attitude + 11.64
```
This is how we can estimate what the test score of any student is, if we know his or her attitude. The R-squared is 0.19, which is low, which means that the data points are scattered quite far away from the estimation line and the fit is poor. However there appears to be a weak correlation.

## EXERCISE 5
```{r eval=FALSE}
par(mfrow = c(2,2))
plot(my_model2,which=(c(1,2,5)))
```
[The output plots are here.](Diagnostics.png) Residuals VS Fitted supports my theory that Points are fitted okayish with attitude-curve. There are approximately some amount of data point both up and down half-planes with respect to the fit line. However it is easily seen that many point lie far from the fit line.

Normal Q-Q supports the assumption that the errors are distributed along the Gaussian normal distribution, since almost all the points appear to follow a straight line.

Residuals VS Leverage plot is good, since there appears not to be significant outliers in the data which would have unreasanobly large leverage on the fitting of the model.


